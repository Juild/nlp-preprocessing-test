## Daily

- Tuesday, 7th June: Read article and understand what theoretical concepts I need to understand before start building the RNN. (Look for the learning references I need)
- Wednesday and Thursday 8th, 9th June:  Use the references in the above step to understand these theoretical concepts.



## Manuscript

### Introduction

- General introduction to Machine Learning
- Emphasis on Unsupervised ML
- Deep Learning (introduction to DNN)
- Intro to NLP
- DNN for NLP
-  Introduction to the "Experiment"

### Experiment

- Build the RNN (Tensorflow/Pytorch)
- Train the RNN
- Analyze results

### Conclusions

## Buzz terms

- **Transformer**

- **BERT-like objective**

- **seq2seq model:** 

  - Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In *Advances in neural information processing systems*, pages 3104–3112, 2014.
  - Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. *arXiv preprint arXiv:1409.0473*, 2014.
  - Probabilistic ML book.
  - The 100-page ML book.

  

- **Attention**
- **Encoder and Decoder**:
  - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *Advances in neural information processing systems*, pages 5998–6008, 2017.
  - Probabilistic ML book.
- Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. *arXiv preprint arXiv:1901.07291*, 2019.

Next things to do: (These is the mininaml theory I think I should be familiar with before starting to develop the architecture of the model in TF and pre-processing the training data in BQ)

- Read the following sections from ProbML (approx 300min ~ 5h):

  - RNN (emphasis in seq2seq models) 100 min
  - Attention section (overview) 70 min
  - Transformers (until putting it all together) 70 min
  - Language models and unsupervised representation learning (BERT) 30 min

- Read Hugging face blog regarding [encoder-decoder transformer models](https://huggingface.co/blog/encoder-decoder)

- Read attention paper [Attention is all you need](https://arxiv.org/abs/1706.03762)

  